{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hsIFsnQBXsj",
        "colab_type": "text"
      },
      "source": [
        "**Лабораторная работа №7.** Рекуррентные нейронные сети для анализа текста\n",
        "\n",
        "\n",
        "1.   Загрузите данные. Преобразуйте текстовые файлы во внутренние структуры данных, которые используют индексы вместо слов.\n",
        "\n",
        "2.   Реализуйте и обучите двунаправленную рекуррентную сеть (LSTM или GRU). Какого качества классификации удалось достичь?\n",
        "\n",
        "3.   Используйте индексы слов и их различное внутреннее представление (word2vec, glove). Как влияет данное преобразование на качество классификации?\n",
        "\n",
        "4.   Поэкспериментируйте со структурой сети (добавьте больше рекуррентных, полносвязных или сверточных слоев). Как это повлияло на качество классификации?\n",
        "\n",
        "5.   Используйте предобученную рекуррентную нейронную сеть (например, DeepMoji или что-то подобное).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlPcPzmed_JX",
        "colab_type": "code",
        "outputId": "c1803770-eca6-4b2b-9a5e-5818335c8e92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "try:\n",
        "  import wget\n",
        "except: \n",
        "  !pip install wget\n",
        "  import wget "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=4cd6f942b03df0d209ec08406371b48342c2530fad28f2c9334fb060819bbc4c\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fLv4KcEeJDk",
        "colab_type": "code",
        "outputId": "a0e9a1c3-ed75-4f7d-9a96-85a1996a2d98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!wget  http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-19 12:45:15--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  20.3MB/s    in 7.2s    \n",
            "\n",
            "2020-04-19 12:45:23 (11.1 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H83Jf7aIeOCP",
        "colab_type": "code",
        "outputId": "b3317ab6-11af-4856-c5a3-a8c6ca37e8d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!ls data/aclImdb/train\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "labeledBow.feat  pos\tunsupBow.feat  urls_pos.txt\n",
            "neg\t\t unsup\turls_neg.txt   urls_unsup.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8HX3jmmeaDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tarfile\n",
        "with tarfile.open('aclImdb_v1.tar.gz') as tar:\n",
        "  tar.extractall('data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtFjQuqzCHOf",
        "colab_type": "text"
      },
      "source": [
        "**Задание 1.**\n",
        "Загрузите данные. Преобразуйте текстовые файлы во внутренние структуры данных, которые используют индексы вместо слов.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zPD2f8Cljr3",
        "colab_type": "code",
        "outputId": "eb4bd056-364c-4074-f797-cc2442b3f61d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import string\n",
        "\n",
        "def get_dfs(start_path):\n",
        "\n",
        "  df = pd.DataFrame(columns=['text', 'sent'])\n",
        "  text = []\n",
        "  sent = []\n",
        "  for p in ['pos','neg']:\n",
        "    path=os.path.join(start_path, p)\n",
        "    files = [f for f in os.listdir(path)\n",
        "             if os.path.isfile(os.path.join(path,f))]\n",
        "    for f in files:\n",
        "      with open (os.path.join(path, f), \"r\") as myfile:\n",
        "        # replace carriage return linefeed with spaces\n",
        "        text.append(myfile.read()\n",
        "                    .replace(\"\\n\", \" \")\n",
        "                    .replace(\"\\r\", \" \"))\n",
        "        # convert positive reviews to 1 and negative reviews to zero\n",
        "        sent.append(1 if p == 'pos' else 0)\n",
        "\n",
        "  df['text']=text\n",
        "  df['sent']=sent\n",
        "  #This line shuffles the data so you don't end up with contiguous\n",
        "  #blocks of positive and negative reviews\n",
        "  df = df.sample(frac=1).reset_index(drop=True)      \n",
        "  return df\n",
        "\n",
        "train_df = get_dfs (\"data/aclImdb/train/\")\n",
        "test_df = get_dfs (\"data/aclImdb/test/\")\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Don't really know where to start with one of t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Watching Smother was perhaps the longest not-q...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I hadn't heard of this film until I read an ar...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Perhaps one of the most overrated so-called ho...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I like my Ronald Colman dashing and debonair, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  sent\n",
              "0  Don't really know where to start with one of t...     0\n",
              "1  Watching Smother was perhaps the longest not-q...     0\n",
              "2  I hadn't heard of this film until I read an ar...     0\n",
              "3  Perhaps one of the most overrated so-called ho...     0\n",
              "4  I like my Ronald Colman dashing and debonair, ...     1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLyAH6l7mJvv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "NUM_WORDS=20000\n",
        "SEQ_LEN=100\n",
        "EMBEDDING_SIZE=100\n",
        "BATCH_SIZE=128\n",
        "EPOCHS=5\n",
        "THRESHOLD=0.5\n",
        "\n",
        "#create tokenizer for our data\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=NUM_WORDS, oov_token='<UNK>')\n",
        "tokenizer.fit_on_texts(train_df['text'])\n",
        "\n",
        "\n",
        "#convert text data to numerical indexes\n",
        "train_seqs=tokenizer.texts_to_sequences(train_df['text'])\n",
        "test_seqs=tokenizer.texts_to_sequences(test_df['text'])\n",
        "\n",
        "#pad data up to SEQ_LEN (note that we truncate if there are more than SEQ_LEN tokens)\n",
        "train_seqs=tf.keras.preprocessing.sequence.pad_sequences(train_seqs, maxlen=SEQ_LEN, padding=\"post\")\n",
        "test_seqs=tf.keras.preprocessing.sequence.pad_sequences(test_seqs, maxlen=SEQ_LEN, padding=\"post\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOmOo4EKCBoI",
        "colab_type": "text"
      },
      "source": [
        "**Задание 2.**\n",
        "Реализуйте и обучите двунаправленную рекуррентную сеть (LSTM или GRU). Какого качества классификации удалось достичь?\n",
        "\n",
        "79 процентов\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2r8dJcRn-8F",
        "colab_type": "code",
        "outputId": "b4ec4577-eeff-430f-d071-91f546c16326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Embedding(NUM_WORDS, EMBEDDING_SIZE),\n",
        "  tf.keras.layers.GlobalAveragePooling1D(),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_17 (Embedding)     (None, None, 100)         2000000   \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_2 ( (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 2,000,101\n",
            "Trainable params: 2,000,101\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcPyygKboG_V",
        "colab_type": "code",
        "outputId": "7955c592-14b2-45ce-d87d-dc19b2af9fc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "es = tf.keras.callbacks.EarlyStopping(monitor='accuracy', mode='max')\n",
        "callbacks=[es]\n",
        "history = model.fit(train_seqs, train_df['sent'].values, \n",
        "                    batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.2, callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "157/157 [==============================] - 4s 24ms/step - loss: 0.6551 - accuracy: 0.7160 - val_loss: 0.5918 - val_accuracy: 0.7788\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 4s 23ms/step - loss: 0.5192 - accuracy: 0.8109 - val_loss: 0.4643 - val_accuracy: 0.8226\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 4s 23ms/step - loss: 0.4076 - accuracy: 0.8515 - val_loss: 0.3969 - val_accuracy: 0.8402\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 4s 23ms/step - loss: 0.3402 - accuracy: 0.8749 - val_loss: 0.3609 - val_accuracy: 0.8506\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 4s 23ms/step - loss: 0.2960 - accuracy: 0.8913 - val_loss: 0.3415 - val_accuracy: 0.8558\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pRGQNXJoOkE",
        "colab_type": "code",
        "outputId": "98540831-b401-4c60-c35e-1af27ae922d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(test_seqs, test_df['sent'].values)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 2s 2ms/step - loss: 0.3542 - accuracy: 0.8465\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3541618883609772, 0.8464800119400024]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcvWbmmAs4Bf",
        "colab_type": "code",
        "outputId": "cddc7024-8ea9-4ee4-8866-9fae30e49d16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Embedding(NUM_WORDS, EMBEDDING_SIZE),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(6)),\n",
        "  tf.keras.layers.Dropout(0.25),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_seqs, train_df['sent'].values, \n",
        "                    batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.2, callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_28 (Embedding)     (None, None, 100)         2000000   \n",
            "_________________________________________________________________\n",
            "bidirectional_14 (Bidirectio (None, 12)                5136      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 12)                0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 1)                 13        \n",
            "=================================================================\n",
            "Total params: 2,005,149\n",
            "Trainable params: 2,005,149\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 6s 37ms/step - loss: 0.5797 - accuracy: 0.7204 - val_loss: 0.4087 - val_accuracy: 0.8252\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 5s 33ms/step - loss: 0.3317 - accuracy: 0.8735 - val_loss: 0.3695 - val_accuracy: 0.8436\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 5s 33ms/step - loss: 0.2371 - accuracy: 0.9179 - val_loss: 0.4023 - val_accuracy: 0.8110\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I08usV3taIA",
        "colab_type": "code",
        "outputId": "542dd26c-cc90-429b-f5aa-83e0e03cdbc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(test_seqs, test_df['sent'].values)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 5s 6ms/step - loss: 0.4252 - accuracy: 0.7986\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4252145290374756, 0.7986000180244446]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L26PobsmCb_t",
        "colab_type": "text"
      },
      "source": [
        "**Задание 3.**\n",
        "Используйте индексы слов и их различное внутреннее представление (word2vec, glove). Как влияет данное преобразование на качество классификации?\n",
        "\n",
        "Качество модели повысилось до 82 процентов\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoP_8R3qvfHr",
        "colab_type": "code",
        "outputId": "fb9b7132-93bc-4f84-f742-6ded220c749c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!wget  http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-19 14:01:01--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-04-19 14:01:01--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-04-19 14:01:02--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.16MB/s    in 6m 29s  \n",
            "\n",
            "2020-04-19 14:07:31 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eakVUwbJwN29",
        "colab_type": "code",
        "outputId": "9a6236c5-fbc7-4c5a-93df-24e942fe37e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!unzip glove.6B.zip\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "aclImdb_v1.tar.gz  glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip\n",
            "data\t\t   glove.6B.200d.txt  glove.6B.50d.txt\t sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vT8N5iSxPri",
        "colab_type": "code",
        "outputId": "b8e8eda8-ed61-41e8-c120-812c4eb84d09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "embeddings_index = {}\n",
        "with open('glove.6B.100d.txt') as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbZq4jSHxgWd",
        "colab_type": "code",
        "outputId": "7d24d8f7-9c97-4b45-ab81-f74dc413db7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "num_words = min(NUM_WORDS, len(word_index) + 1)\n",
        "embedding_matrix = np.zeros((num_words, 100))\n",
        "for word, i in word_index.items():\n",
        "    #print (word)\n",
        "    if i >= NUM_WORDS:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 88583 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "705wgXMt7Ft8",
        "colab_type": "code",
        "outputId": "99919cb9-4ba2-479c-e570-39c8804711d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Embedding(NUM_WORDS, EMBEDDING_SIZE),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(60)),\n",
        "  tf.keras.layers.Dropout(0.25),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_seqs, train_df['sent'].values, \n",
        "                    batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.2, callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_30 (Embedding)     (None, None, 100)         2000000   \n",
            "_________________________________________________________________\n",
            "bidirectional_16 (Bidirectio (None, 120)               77280     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 120)               0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 1)                 121       \n",
            "=================================================================\n",
            "Total params: 2,077,401\n",
            "Trainable params: 77,401\n",
            "Non-trainable params: 2,000,000\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 3s 19ms/step - loss: 0.5761 - accuracy: 0.6877 - val_loss: 0.4987 - val_accuracy: 0.7614\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 2s 15ms/step - loss: 0.4688 - accuracy: 0.7785 - val_loss: 0.4474 - val_accuracy: 0.7906\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 2s 15ms/step - loss: 0.4399 - accuracy: 0.7943 - val_loss: 0.4179 - val_accuracy: 0.8090\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 2s 15ms/step - loss: 0.4073 - accuracy: 0.8120 - val_loss: 0.4046 - val_accuracy: 0.8142\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 2s 14ms/step - loss: 0.3926 - accuracy: 0.8214 - val_loss: 0.3955 - val_accuracy: 0.8268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P9SwNH77laz",
        "colab_type": "code",
        "outputId": "003fd516-399e-4ca5-e9d7-e0e1e8810821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(test_seqs, test_df['sent'].values)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 5s 6ms/step - loss: 0.3995 - accuracy: 0.8204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3994566798210144, 0.820360004901886]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    }
  ]
}